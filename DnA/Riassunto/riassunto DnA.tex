\documentclass[a4paper]{book}
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{color}
\usepackage{listings}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\newenvironment{mytheorem}[1]{\subsubsection*{Theorem #1}}{\begin{flushright}$\blacksquare$\end{flushright}}

\lstset{frame=tb,
  language=Java,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true
  tabsize=3
}
\author{Sandro Marcon}
\title{Riassunto DnA}
\begin{document}
\chapter{Introduzione}
\section{Notazione O-$\Omega$-$\Theta$ (pag 4)}
Per semplificare l'analisi asintotica degli algoritmi sono state introdotte le 
seguenti notazioni:
$$O(f)=\{ g|\exists a>0: \exists b>0: \forall N \in N : g(N)\leq af(N)+b \}$$
$$\Omega (f)=\{ g|\exists c>0: \exists \mbox{ infinite n } : g(n) \geq cf(n) \}$$
In entrambi i casi si tratta di classi di funzioni. Quando $f \in O(g)$ e $f \in \Omega (g)$ al contempo si dice che $f \in \Theta (g)$. Esistono ulteriori definizioni alternative, ad esempio quelle utilizzate nel Blatt 1. Il seguente teorema può essere utile (dimostrazione nel Blatt 1):
\newtheorem{theorem}{Theorem}
\begin{mytheorem}{1}
Date due funzioni $ f,g: N \rightarrow R^{+} $. Se $ \lim\limits_{x \rightarrow +\infty} \frac{f(n)}{g(n)} $ converge ad una costante $C\geq 0$ allora $f \in O(g)$.
\end{mytheorem}
\section{Algoritmo di Karatsuba (pag 12)}
Un esempio di algoritmo più efficiente per multiplicare due numeri è il seguente:
$$65 * 28 = (2 * 6) * 100 + (2 * 6) * 10 + (5 * 8) * 10 + 5 * 8 + (6-5)*(8-2) * 10 = 1820 $$
In questo modo abbiamo solo 3 multiplicazioni elementari anziché 4. L'algoritmo può essere generalizzato grazie a divide and conquer dividendo ogni numero in due ed applicando l'algoritmo di base. Analizzando il tempo in base al numero delle multimplicazioni otteniamo che impieda circa $O(n^{1,58})$.
\section{Maximum subarray (pag 20)}
Dato un array di numeri il problema consiste nella ricerca del subarray con la somma degli elementi maggiore. Se essa è negativa il risultato è 0. Il metodo più efficiente per ricavare il risultato è il seguente:
\begin{lstlisting}
//A=array da 1, ... n	
max=0	
scan=0
for (i=1; i<=n; i++){	
	scan+=A[i]
	if scan < 0
		scan=0
	if scan > max 
		max=scan
}			
\end{lstlisting}
In questo modo il problema viene risolto in tempo lineare.	
\chapter{Sort}
Per semplicità ammettiamo che si debba sempre ordinare un array (chiamato $a$) contenente $n$ numeri (interi). In ogni caso con questi algoritmi è possibile ordinare qualsiasi oggetto appartenente ad un universo in cui vige un ordine totale.
\section{Sortieren durch Auswahl (pag 82)}
Selection sort consiste nel cercare ogni volta il minimo tra la posizione $i$ e $n$. Una volta trovato esso viene scambiato con l'$i-$tesimo numero. 
\subsubsection*{Esempio}
\[\begin{array}{*{20}{c}}
{}&{15}&2&{43}&{17}&4\\
{\Rightarrow}&2&{15}&{43}&{17}&4\\
{\Rightarrow}&2&4&{43}&{17}&{15}\\
{\Rightarrow}&2&4&{15}&{17}&{43}
\end{array}\]
\subsubsection*{Implementazione}
\begin{lstlisting}
int i,j, min, temp
for i=1:n-1
	min=i
	for j=(i+1):n
		if a[j]$<$ a[min]
			min=j
	temp=a[min]
	a[min]=a[i]
	a[i]=temp
\end{lstlisting}
\subsubsection*{Analisi}

Dal doppio loop si vede semplicemente che l'algoritmo impiega $\Theta (n^2)$ comparazioni e nel peggior caso (i numeri sono ordinati dal più grande al più piccolo) O(n) scambi. Da notare che per trovare il minimo sono necessari almeno $n-1$ confronti (Satz 2.1), quindi l'algoritmo non può andare più veloce di così. 
\section{Sortieren durch Einfügen (pag 85)}
In inglese si chiama insertion sort. Per induzione i numeri fino a $i-1$ sono già ordinati. Il principio consiste di piazzare l'$i-$tesimo elemento al giusto posto, se necessario scalando i restanti a destra di una posizione. Esempio:
\[\begin{array}{*{20}{c}}
{}&2&{15}&/&{43}&{17}&4\\
{\Rightarrow}&2&{15}&{43}&/&{17}&4\\
{\Rightarrow}&2&{15}&{17}&{43}&/&4\\
{\Rightarrow}&2&4&{15}&{17}&{43}&/
\end{array}\]
\subsubsection*{Implementazione}
\begin{lstlisting}
int i,j,t
for i=2:n
	j=i
	t=a[i]
	while a[j-1] > t
		//scala a destra di una posizione
		a[j]=a[j-1]
		j=j-1
	a[j]=t
\end{lstlisting}
Si nota subito che se implementato così l'algoritmo può non fermarsi se $t$ è il più piccolo numero. Serve quindi un elemento di stop, ad esempio inserendo $a[0]=t$ prima del while loop.

\subsubsection*{Analisi}

Nel peggior caso $\Theta (n^2)$ comparazioni ed altrettanti spostamenti. Nel miglior caso $O(n^2)$ comparazioni e spostamenti. Il caso medio rispecchia il peggiore.

\section{Bubblesort (pag 89)}
Il principio di questo algoritmo è semplicissimo: ad ogni iterazione viene scambiato l'elemento $a[i]$ con $a[i+1]$ (chiaramente solo se maggiore). In questo modo l'elemento più grande si sposta verso destra. Esempio:
\[\begin{array}{*{20}{c}}
{}&{15}&2&{43}&{17}&4\\
{\Rightarrow}&2&{15}&{43}&{17}&4\\
{\Rightarrow}&2&{15}&{17}&{43}&4\\
{\Rightarrow}&2&{15}&{17}&4&{43}\\
{\Rightarrow}&2&{15}&4&{17}&{43}\\
{\Rightarrow}&2&4&{15}&{17}&{43}
\end{array}\]
\subsubsection*{Implementazione}
\begin{lstlisting}
do
	flag=true
	for i=1:n-1
	if a[i] > a[i+1] 
		swap(a[i],a[i+1])
		flag=false
while (!flag)
\end{lstlisting}
\subsubsection*{Analisi}

Nel miglior caso, se l'array è già ordinato, abbiamo $n-1$ paragoni e nessuno scambio. Nel caso medio e peggiore l'algoritmo necessita di $\Theta (n^2)$ scambi e paragoni.

\section{Quicksort (pag 92)}
Il principio di quicksort è quello di scegliere un elemento come pivot (nel libro è sempre l'ultimo, ma ne basta uno casuale) e dividere l'array in due. A sinistra del pivot si trovano tutti gli elementi minori, mentre a destra quelli maggiori. Dopodiché viene richiamato l'algoritmo in modo ricorsivo sui due blocchi. L'algoritmo può essere eseguito "in situ", vale a dire che non serve spazio extra per salvare i dati (chiaramente un numero costante di variabili temporanee è permesso). Si usa quindi un solo array e si aggiungono due argomenti al metodo, ad esempio $l$ ed $r$, ad indicare l'intervallo in cui eseguire quicksort. Per dividere i numeri rispettivamente a destra ed a sinistra del pivot si può semplicemente far scorrere verso il centro due puntatori (chiamati $i$ e $j$) che partono rispettivamente alla posizione $l$ e $r$. Quando $a[i] \geq pivot$ e $a[j] \leq pivot$, $a[i]$ ed $a[j]$ vengono scambiati. Alla fine, quando $i=j$, l'elemento $a[i]$ viene scambiato col pivot e viene richiamato l'algoritmo da $l$ ad $i-1$ e da $i+1$ a $r$ (il pivot si trova già alla posizione giusta). Di seguito un esempio:
\subsubsection*{Esempio}
\begin{center}
array: 5 7 3 1 6 4\\
quicksort(array, 1, 6)
\end{center}
\[\begin{array}{*{20}{c}}{}&{}&{}&5&7&3&1&6&{(4)}& \leftarrow &{{\text{pivot}}}\\{}&{}&{}& \uparrow &{}&{}& \uparrow &{}&{}&{}&{}\\{}&{}&{}&i&{}&{}&j&{}&{}&{}&{}\\{}&{}& \Rightarrow &1&7&3&5&6&{(4)}&{}&{}\\{}&{}&{}&{}& \uparrow & \uparrow &{}&{}&{}&{}&{}\\{}&{}&{}&{}&i&j&{}&{}&{}&{}&{}\\{}&{}& \Rightarrow &1&3&7&5&6&{(4)}&{}&{}\\{}&{}&{}&{}&{}& \uparrow &{}&{}&{}&{}&{}\\{}&{}&{}&{}&{}&{i,j}&{}&{}&{}&{}&{}\\{}&{}& \Rightarrow &1&3&{(4)}&5&6&7&{}&{}\end{array}\]
\begin{center}quicksort(array, 1, 2), quicksort(array, 3, 6)\end{center}
\subsection*{Implementazione}

\begin{lstlisting}
quicksort(int[ ] a, int l, int r)
	if r>l
		i=l-1
		j=r
		v=a[r] //pivot
		while (true)
			do
				i+=1
			until a[i]>=v
			do
				j-=1
			until  a[j]<=v
			if i>=j
				break
			swap(i, j) //scambia a[i] con a[j]
	 	swap(i,r)
		quicksort(a, l, i-1)
		quicksort(a, i+1, r)
\end{lstlisting}    
      
\subsection*{Analisi}
Nel peggior caso, ad esempio se la sequenza è già ordinata o più in generale se il pivot è sempre l'elemento maggiore o minore, quicksort impiega $O(n^2)$ comparazioni, poiché ad ogni ricorsione c'è un solo elemento in meno. In questo caso avverrebbero $O(n)$ spostamenti. Nel miglior caso l'algoritmo impiega un tempo di $\Theta (n\log (n))$, poiché l'albero delle ricorsioni ha un'altezza logaritmica. Lo stesso ragionamento vale per il numero di spostamenti. A pagina 99 c'è una lunga dimostrazione per induzione che mostra che anche il caso medio ha lo stesso tempo del miglior caso. Nonostante l'algoritmo sia in situ, anche le ricorsioni occupano un determinato spazio nella memoria, vale a dire $\Omega (n)$ chiamate. Rendendo l'algoritmo semi-iterativo si possono ottenere solo $O(\log (n))$ chiamate. Si deve semplicemente richiamare in modo ricorsivo quicksort sul più piccolo sub-array, mentre si completa l'altra parte in modo iterativo (algoritmo illustrato a pagina 100). Alla pagina successiva è invece mostrato come fare a rendere quicksort totalmente iterativo (personalmente non ho capito molto). 
            
\subsection*{Varianti di quicksort}
Per evitare un tempo di $O(n^2)$ nel caso di un array già ordinato sono state pensate alcune varianti dell'algoritmo, le quali concernono solamente la scelta del pivot. La prima è la cosiddetta 3-Median-Strategie (median of three values strategy), che consiste semplicemente nel prendere tre elementi campione, rispettivamente a destra, sinistra e nel mezzo, e scegliere come pivot il valore medio tra questi.
L'altra strategia si chiama Zufalls-Strategie (randomized quicksort) e consiste nel scegliere il pivot casualmente. Chiaramente può ancora avvenire un caso in cui necessita di un tempo quadratico, ma non esiste più una successione di numeri per cui questo accade. Una variante per rendere l'algoritmo glatt (smooth), cioè che impiega in media $O(n \log (n))$ comparazioni e  solo $O(n)$ quando l'input consiste in n elementi uguali, è la seguente. Praticamente gli elementi che sono uguali al pivot vengono spostati all'estrema destra o sinistra, dopodiché i due blocchi vengono riportati al centro e l'algoritmo prosegue come di consueto. L'implementazione ed una spiegazione più dettagliata possono essere trovate a pagina 104.

\section{Heapsort (pag 106)}
Heapsort si basa su insertion sort, ma invece di cercare il minimo/massimo ogni volta, che costa $O(n)$ paragoni, ci si affida ad un Heap (un tipo di albero binario). L'algoritmo è semplice ma efficace: finché l'albero non è vuoto si allontana la radice, nonché l'elemento massimo, e lo si scambia con l'ultimo elemento. Dopodiché tramite un indice si riduce l'albero di un elemento e si restaurano (tramite un metodo chiamato versickern) le sue proprietà (richiede un tempo di $O(\log (n))$). Rimane solo la parte iniziale, cioè la costruzione di un Heap a partire da un array qualsiaisi. Un metodo efficiente consiste nel eseguire il metodo "versickern" sulla sequenza delle chiavi $k_{\lfloor \frac{n}{2} \rfloor}, k_{\lfloor \frac{n}{2} \rfloor - 1}, ... , k_1$. Esempio:

\[\begin{array}{*{20}{c}}
{}&{2}&1&{5}&{3}&4&8&7&6\\
{\Rightarrow}&2&{1}&{5}&{6}&4&8&7&3\\
{\Rightarrow}&2&{1}&{8}&{6}&4&5&7&3\\
{\Rightarrow}&2&{6}&{8}&3&{4}&5&7&1\\
{\Rightarrow}&8&{6}&7&{3}&{4}&5&2&1\\
\end{array}\]

\subsection*{Implementazione}
\begin{lstlisting}
//costruiamo l'heap
for (i=n/2; i>=1; i--)
	versickere(a,i,n) //array a da i a N
//heapsort	
for (i=n	; i>=2; i--)
	swap(a[1],a[i])
	versickere(a,1,i-1)
\end{lstlisting}                                                                          
Dove versickere è:
\begin{lstlisting}
versickere(int[] a, int i, int m)
	int j
	while 2*i<=m //a[i] ha un figlio a sinistra
		j=2*i
		if j<m //a[i] ha anche un figlio a destra
			if a[j]<a[j+1]
				j=j+1
			//ora a[j] e' il figlio con un valore maggiore
			if a[i]<a[j]
				swap(a[i],a[j])
				i=j //va avanti a restaurare
			else
				i=m //fine		
\end{lstlisting}                                                                                                 
\subsection*{Analisi}
La costruzione dell'heap avviene in tempo lineare. Il resto dell'algoritmo prevede $n$ volte il metodo versickere, quindi in complesso l'algoritmo richiede $O(n \log (n))$ comparazioni e spostamenti nel peggior caso. Da notare che non occupa spazio extra, quindi è completamente "in situ". L'unico problema è che non è stabile, vale a dire che la posizione relativa di chiavi con lo stesso valore inizialmente adiacenti può cambiare. 

\section{Mergesort (pag 112)}
Anche mergesort si basa su divide and conquer. Si basa sul fatto che due blocchi dell'array sono già ordinati, quindi possono semplicemente essere uniti in tempo lineare, lasciando scorrere due indici nelle rispettive parti e salvando i valori in ordine crescente in un array temporaneo.

\subsection*{Esempio}
$A=\{2, 1, 3, 9, 5, 4\}$. Esso viene dunque diviso in due, vale a dire in $A_1 =\{2,1,3\}$ e $A_2 =\{9,5,4\}$. Per ricorsione essi vengono ordinati, quindi rimane da unire $A_1 =\{1,2,3\}$ e $A_2 =\{4,5,9\}$. Il passo merge prevede l'unione dei due insiemi, quindi si ottiene il risultato finale.

\subsection*{Implementazione}
\begin{lstlisting}
mergesort (int[] a, int l, int r)
	if l<r //altrimenti caso base: array rimane invariato
		m=(l+r)/2 //posizione media
		mergesort (a, l, m)
		mergesort (a, m+1, r)
		merge (a, l, m, r)
		
merge (int[] a, int l,m,r)
	i=l
	j=m+1
	k=l
	while (i<=m && j<=r)
		if a[i]<a[j]
			b[k]=a[i]
			i++
		else
			b[k]=a[j]
			j++
		k++
	if i>m //prima successione esautita
		for h=j:r
			b[k+h-j]=a[h]
	else
		for h=i:m
			b[k+h-i]=a[h]
	for h=l:r
		a[h]=b[h] //salva nuovamente nell'array originale									
\end{lstlisting}
\subsection*{Analisi}
Merge impiega sempre $O(n)$ spostamenti e comparazioni. Dato che l'array viene sempre diviso in due, il metodo merge viene chiamato $\log(n)$ volte. In totale l'algoritmo impiega sempre $\Theta (n \log(n))$. L'unico problema è che serve sempre $O(n)$ spazio extra per salvare l'array di supporto.
\subsection*{Mergesort iterativo}
L'algoritmo può anche essere reso totalmente iterativo. Esso si chiama "reines 2-Wege-Mergesort" (straight 2-way merge sort). L'idea è quella di unire tramite il metodo merge intervalli sempre più grandi (esattamente il doppio) fino ad avere l'intero array completamente ordinato. Esempio:
\[\begin{array}{*{20}{c}}
{}&{2}&1&{3}&{9}&6&5\\
{\Rightarrow}&1&{2|}&{3}&{9|}&5&{6|}\\
{\Rightarrow}&1&{2}&{3}&{9|}&5&6\\
{\Rightarrow}&1&{2}&{3}&5&{6}&{9|}\\
\end{array}\]
\begin{lstlisting}
straightmergesort(int[] a, int l,r)
	int size, ll, mm, rr
	size=1
	while size<r-l+1
		rr=l-1 //gli elementi fino ad a[rr] incluso sono a posto
		while rr+size<r //fino che tutti non sono elaborati
			ll=rr+1 //bordo sinistro successione 1
			mm=ll+size-1 //bordo desto successione 1
			if mm+size<=r
				rr=mm+size
			else
				rr=r
			merge (a, ll, mm, rr)
		size=2*size				
\end{lstlisting}
Le performance sono identiche a prima, ma il metodo è completamente iterativo.
\subsection*{Mergesort naturale}
L'unica differenza da straight 2-way merge sort è che al posto di unire successioni di lunghezza arbitraria sfruttiamo successioni già ordinate alla partenza. Ad esempio:
\[\begin{array}{*{20}{c}}
{}&{2}&1&{3}&{6}&9&5\\
{\Rightarrow}&{2|}&{1}&{3}&{6}&{9|}&{5}\\
{\Rightarrow}&1&{2}&{3}&{6}&{9|}&{6|}\\
{\Rightarrow}&1&{2}&{3}&5&{6}&{9|}\\
\end{array}\]
A livello di implementazione dobbiamo soltanto aggiungere la ricerca delle successioni già ordinate.
\begin{lstlisting}
straightmergesort(int[] a, int l,r)
	int ll, mm, rr
	do
		rr=l-1
		while rr<r
			ll=rr+1
			mm=ll	
			while (mm<r && a[mm+1]>=a[mm])
				mm++
			if mm<r
				while (rr<r && a[rr+1]>=a[rr])
					rr++
				merge (a, ll, mm, rr)
			else
				rr=mm
	until ll=l								
\end{lstlisting}
In questo modo cambiano anche le performance dell'algoritmo. Compare difatti un best case, ad esempio quando l'input è già ordinato. In questo caso avremmo $\Theta (n)$ comparazioni e 0 spostamenti. Il caso medio ed il peggior caso rispecchiano invece straight merge sort, nonché le tempistiche della prima analisi.
\section{Radixsort (pag 121)}
Radixsort si basa sulla comparazione delle cifre dei rispettivi numeri. Noi tratteremo solo la forma duale, ma il tutto funziona con qualsiasi forma, compresa quella decimale. Ammettiamo sempre che esista una funzione $z(i,k)$ che ritorna l'$i-$tesima cifra a partire dalla meno significativa del numero $k$. Ad esempio $z(0,517)=7$ e $z(2, 517)=5$.
\subsection{Radix-exchange sort}
L'algoritmo segue un principio abbastanza simile a quicksort: tutti gli elementi il cui $i-$tesimo bit è 0 vanno a sinistra, mentre se è uno vanno a destra. Il modo più semplice è quello, come in quicksort, di far scorrere due puntatori verso il centro e scambiare i numeri che si trovano al posto sbagliato. Dopodiché si richiama l'algoritmo sull' $(i-1)-$tesimo bit e sulle due parti dell'array.
\subsubsection*{Esempio}
\[\begin{array}{*{20}{c}}
{}&{1011}&{0010}&{1101}&{0011}&{|3}\\
{\Rightarrow}&{0011}&{0010;}&{1101}&{1011;}&{|2}\\
{\Rightarrow}&{0011}&{0010;}&{1011;}&{1101;}&{|1}\\
{\Rightarrow}&{0011}&{0010;}&{1011;}&{1101;}&{|0}\\
{\Rightarrow}&{0010}&{0011}&{1011}&{1101}
\end{array}\]
\subsubsection*{Implementazione}
\begin{lstlisting}
radixexchangesort(int[] a, int l,r,b)
	int i,j
	if r>l
		i=l-1
		j=r+1
		while (true)
			do
				i++
			until
				z(b, a[i])=1 || i>=j
			do
				j--
			until
				z(b, a[j])=0 || i>=j
			if i>=j
				break
			swap (a[i], a[j])
		if b>0
			radixexchangesort(a, l, i-1, b-1)
			radixexchangesort(a, i, r, b-1)						
\end{lstlisting}
\subsection*{Analisi}
L'algoritmo viene richiamato al massimo $b$ volte ed ogni volta impiega $O(n)$ paragoni e movimenti. In totale impiega dunque $O(bn)$ (tempo pseudo polinomiale). Se $b> \log (n)$ radixsort non è l'algoritmo adeguato al problema. 
\end{document}